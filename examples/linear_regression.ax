# Linear Regression in Axiom
# Demonstrates Data Science capabilities

struct LinearRegression:
    weights: Tensor[f64]
    bias: f64
    learning_rate: f64
    
    fn __init__(self, learning_rate: f64 = 0.01):
        self.learning_rate = learning_rate
        self.bias = 0.0
    
    fn fit(self, X: Tensor[f64], y: Tensor[f64], epochs: i32 = 1000):
        let n_samples = X.shape[0]
        let n_features = X.shape[1]
        
        # Initialize weights
        self.weights = Tensor.zeros((n_features,))
        
        for epoch in 0..epochs:
            # Forward pass
            let y_pred = X @ self.weights + self.bias
            
            # Compute gradients
            let error = y_pred - y
            let dw = (2.0 / n_samples) * (X.T @ error)
            let db = (2.0 / n_samples) * error.sum()
            
            # Update weights
            self.weights -= self.learning_rate * dw
            self.bias -= self.learning_rate * db
            
            if epoch % 100 == 0:
                let loss = (error ** 2).mean()
                print(f"Epoch {epoch}: Loss = {loss}")
    
    fn predict(self, X: Tensor[f64]) -> Tensor[f64]:
        return X @ self.weights + self.bias
    
    fn score(self, X: Tensor[f64], y: Tensor[f64]) -> f64:
        let y_pred = self.predict(X)
        let ss_res = ((y - y_pred) ** 2).sum()
        let ss_tot = ((y - y.mean()) ** 2).sum()
        return 1.0 - (ss_res / ss_tot)

fn main():
    # Generate synthetic data
    let X = Tensor.randn((100, 3))
    let true_weights = Tensor([2.0, -1.5, 0.5])
    let y = X @ true_weights + 3.0 + Tensor.randn((100,)) * 0.1
    
    # Train model
    let model = LinearRegression(learning_rate=0.1)
    model.fit(X, y, epochs=500)
    
    # Evaluate
    let r2 = model.score(X, y)
    print(f"\nRÂ² Score: {r2}")
    print(f"Learned weights: {model.weights}")
    print(f"Learned bias: {model.bias}")
    print(f"True weights: {true_weights}")
    print(f"True bias: 3.0")
